---
title: Kubernetes and the router
description: Using router images with kubernetes
---

import { Link } from 'gatsby';

## Sample Kubernetes Configuration

> Note: The Apollo Router is made available under the Elastic License v2.0 (ELv2).  This applies to its source code and all distributions, including versions installed via Helm charts.  Read [our licensing page](https://www.apollographql.com/docs/resources/elastic-license-v2-faq/) for more details.

### Helm

[Helm](https://helm.sh) is the package manager for kubernetes.

There is a complete [helm chart definition](https://github.com/apollographql/router/tree/v1.30.1/helm/chart/router) in the repo which illustrates how to use helm to deploy the router in kubernetes.

In both the following examples, we are using helm to install the router:
 - into namespace "router-deploy" (create namespace if it doesn't exist)
 - with helm install name "router-test"
 - with support for prometheus enabled

#### Using helm chart from our Open Container Initiative (OCI) registry

Starting with release 0.14.0, each time we release the router, we'll release
our helm chart and store it in the same OCI registry in which we store our
router docker images.

You can use helm to install charts from an OCI registry as follows:

```bash
helm install --set router.configuration.telemetry.metrics.prometheus.enabled=true --set managedFederation.apiKey="REDACTED" --set managedFederation.graphRef="REDACTED" --create-namespace --namespace router-deploy router-test oci://ghcr.io/apollographql/helm-charts/router --version 1.0.0-rc.8 --values router/values.yaml
```

For more details about using helm with OCI based registries, see [here](https://helm.sh/docs/topics/registries/)

#### Using helm chart from your filesystem

You would run this command from "repo"/helm/chart directory.

(where "repo" is the directory containing your checked out router github repository)

```bash
helm install --set router.configuration.telemetry.metrics.prometheus.enabled=true --set managedFederation.apiKey="REDACTED" --set managedFederation.graphRef="REDACTED" --create-namespace --namespace router-deploy router-test router --values router/values.yaml
```

Once executed, you can check the status of the helm deploy:

```bash
helm list --namespace router-deploy
```

### Kubernetes Configuration

If you aren't familiar with helm, the following example illustrates how you could do the same thing manually or as a base for kustomize.

Note: This example is generated using the helm template capability to generate the required kubernetes configuration from our helm chart. After generation, it is edited to remove the Helm management annotations.

```yaml
---
# Source: router/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-router
  labels:
    helm.sh/chart: router-1.30.1
    app.kubernetes.io/name: router
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v1.30.1"
    app.kubernetes.io/managed-by: Helm
---
# Source: router/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: "release-name-router"
  labels:
    helm.sh/chart: router-1.30.1
    app.kubernetes.io/name: router
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v1.30.1"
    app.kubernetes.io/managed-by: Helm
data:
  managedFederationApiKey: "UkVEQUNURUQ="
---
# Source: router/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-router
  labels:
    helm.sh/chart: router-1.30.1
    app.kubernetes.io/name: router
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v1.30.1"
    app.kubernetes.io/managed-by: Helm
data:
  configuration.yaml: |
    health_check:
      listen: 0.0.0.0:8088
    supergraph:
      listen: 0.0.0.0:80
    telemetry:
      metrics:
        common:
          resources:
            service.name: release-name-router
        prometheus:
          enabled: true
          listen: 0.0.0.0:9090
          path: /metrics
---
# Source: router/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-router
  labels:
    helm.sh/chart: router-1.30.1
    app.kubernetes.io/name: router
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v1.30.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
    - port: 8088
      targetPort: health
      protocol: TCP
      name: health
  selector:
    app.kubernetes.io/name: router
    app.kubernetes.io/instance: release-name
---
# Source: router/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-router
  labels:
    helm.sh/chart: router-1.30.1
    app.kubernetes.io/name: router
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v1.30.1"
    app.kubernetes.io/managed-by: Helm
  
  annotations:
    prometheus.io/path: /metrics
    prometheus.io/port: "9090"
    prometheus.io/scrape: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: router
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: router
      labels:
        app.kubernetes.io/name: router
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: release-name-router
      securityContext:
        {}
      containers:
        - name: router
          securityContext:
            {}
          image: "ghcr.io/apollographql/router:v1.30.1"
          imagePullPolicy: IfNotPresent
          args:
            - --hot-reload
            - --config
            - /app/configuration.yaml
          env:
            - name: APOLLO_KEY
              valueFrom:
                secretKeyRef:
                  name: "release-name-router"
                  key: managedFederationApiKey
                  optional: true
            - name: APOLLO_GRAPH_REF
              value: REDACTED
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
            - name: health
              containerPort: 8088
              protocol: TCP
          lifecycle:
            {}
          livenessProbe:
            httpGet:
              path: "/health?live"
              port: 8088
            initialDelaySeconds: 0
          readinessProbe:
            httpGet:
              path: "/health?ready"
              port: 8088
            initialDelaySeconds: 0
          resources:
            {}
          volumeMounts:
            - name: router-configuration
              mountPath: /app/
              readOnly: true
      volumes:
        - name: router-configuration
          configMap:
            name: release-name-router
      terminationGracePeriodSeconds: 30
---
# Source: router/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "release-name-router-test-connection"
  labels:
    helm.sh/chart: router-1.30.1
    app.kubernetes.io/name: router
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v1.30.1"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: netcat
      image: busybox
      command: ['nc']
      args: ['-vz','-w','1','release-name-router:80']
  restartPolicy: Never
```

## The health endpoint

The router supports a health endpoint. You can see from the examples above how it can be used in a kubernetes deployment.

If you had a router running on port 8088 on your localhost, you could exercise the health endpoint as follows:

```bash
curl "http://localhost:8088/health"
{"status":"UP"}
```

If you had a router running on your localhost, with default health-check configuration, you could exercise the health endpoint as follows:

curl "http://localhost:8088/health"

## Using `istio` with the router

The [istio service mesh](https://istio.io/) is a very popular choice for enhanced traffic routing within Kubernetes.

`istio-proxy` pod injection can cause an [issue](https://github.com/apollographql/router/issues/3533) in the router. The router may start executing at the same time that istio is reconfiguring networking for the router pod. This is an issue with `istio`, not the router, and you can resolve it by following the advice in [istio's injection documentation](https://istio.io/latest/docs/ops/common-problems/injection/#pod-or-containers-start-with-network-issues-if-istio-proxy-is-not-ready).

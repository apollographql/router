---
title: In-Memory Caching
subtitle: Configure router caching for query plans and automatic persisted queries
description: Configure in-memory caching for improved performance in Apollo GraphOS Router or Apollo Router Core. Configure query plans and automatic persisted queries caching.
---

Both GraphOS Router and Apollo Router Core use an in-memory LRU cache to store the following data:

- [Generated query plans](#caching-query-plans)
- [Automatic persisted queries (APQ)](#caching-automatic-persisted-queries-apq)
- Introspection responses

You can configure certain caching behaviors for generated query plans and APQ (but not introspection responses).

<Tip>

If you have a GraphOS Enterprise plan, you can also configure a Redis-backed _distributed_ cache that enables multiple router instances to share cached values. For details, see [Distributed caching in the GraphOS Router](/router/configuration/distributed-caching/).

</Tip>

## Performance improvements vs stability

The router is a highly scalable and low-latency runtime. Even with all caching **disabled**, the time to process operations and query plans will be very minimal (nanoseconds to milliseconds) when compared to the overall supergraph request, except in the edge cases of extremely large operations and supergraphs. Caching offers stability to those running a large graph so that your overhead for given operations stays consistent, not that it dramatically improves. If you would like to validate the performance wins of operation caching, check out the [traces and metrics in the router](/router/configuration/telemetry/instrumentation/standard-instruments#performance) to take measurements before and after. In extremely large edge cases though, we have seen the cache save 2-10x time to create the query plan, which is still a small part of the overall request.

## Caching query plans

Whenever your router receives an incoming GraphQL operation, it generates a [query plan](/federation/query-plans/) to determine which subgraphs it needs to query to resolve that operation.

By caching previously generated query plans, your router can _skip_ generating them _again_ if a client later sends the exact same operation. This improves your router's responsiveness.

The GraphOS Router enables query plan caching by default. In your router's [YAML config file](/router/configuration/overview/#yaml-config-file), you can configure the maximum number of query plan entries in the cache like so:

```yaml title="router.yaml"
supergraph:
  query_planning:
    cache:
      in_memory:
        limit: 512 # This is the default value.
```

### Cache warm-up

When loading a new schema, a query plan might change for some queries, so cached query plans cannot be reused.

To prevent increased latency upon query plan cache invalidation, the router precomputes query plans for the most used queries from the cache when a new schema is loaded.

Precomputed plans will be cached before the router switches traffic over to the new schema.

<Tip>

You can also send the header `Apollo-Expose-Query-Plan: dry-run` for [generating query plans at runtime](https://www.apollographql.com/docs/graphos/reference/federation/query-plans#outputting-query-plans-with-headers) which can be used to warm up your cache instances with a custom defined operation list.

</Tip>


By default, the router warms up the cache with 30% of the queries already in cache, but it can be configured as follows:

```yaml title="router.yaml"
supergraph:
  query_planning:
    # Pre-plan the 100 most used operations when the supergraph changes
    warmed_up_queries: 100
```

(In addition, the router can use the contents of the [persisted query list](/router/configuration/persisted-queries) to prewarm the cache. By default, it does this when loading a new schema but not on startup; you can [configure](/router/configuration/persisted-queries#persisted-queries#experimental_prewarm_query_plan_cache) it to change either of these defaults.)

To get more information on the planning and warm-up process use the following metrics (where `<storage>` can be `redis` for distributed cache or `memory`):

* counters:
  * `apollo.router.cache.hit.time.count{kind="query planner", storage="<storage>"}`
  * `apollo.router.cache.miss.time.count{kind="query planner", storage="<storage>"}`

* histograms:
  * `apollo.router.query_planning.plan.duration`: time spent planning queries
    * `planner`: The query planner implementation used (`rust` or `js`)
    * `outcome`: The outcome of the query planning process (`success`, `timeout`, `cancelled`, `error`)
  * `apollo.router.schema.load.duration`: time spent loading a schema
  * `apollo.router.cache.hit.time{kind="query planner", storage="<storage>"}`: time to get a value from the cache
  * `apollo.router.cache.miss.time{kind="query planner", storage="<storage>"}`

* gauges
  * `apollo.router.cache.size{kind="query planner", storage="memory"}`: current size of the cache (only for in-memory cache)
  * `apollo.router.cache.storage.estimated_size{kind="query planner", storage="memory"}`: estimated storage size of the cache (only for in-memory query planner cache)

Typically, we would look at `apollo.router.cache.size` and the cache hit rate to define the right size of the in memory cache,
then look at `apollo.router.schema.load.duration` and `apollo.router.query_planning.plan.duration` to decide how much time we want to spend warming up queries.

#### Cache warm-up with distributed caching

If the router is using distributed caching for query plans, the warm-up phase will also store the new query plans in Redis. Since all Router instances might have the same distributions of queries in their in-memory cache, the list of queries is shuffled before warm-up, so each Router instance can plan queries in a different order and share their results through the cache.

#### Cache warm-up with headers

<MinVersionBadge version="Router v1.61.0" />

With router v1.61.0+  and v2.x+, if you have enabled exposing query plans via `--dev` mode or `plugins.experimental.expose_query_plan: true`, you can pass the `Apollo-Expose-Query-Plan` header to return query plans in the GraphQL response extensions. You must set the header to one of the following values:

- `true`: Returns a human-readable string and JSON blob of the query plan while still executing the query to fetch data.
- `dry-run`: Generates the query plan and aborts without executing the query.

After using `dry-run`, query plans are saved to your configured cache locations. Using real, mirrored, or similar to production operations is a great way to warmup the caches before transitioning traffic to new router instances.

## Caching automatic persisted queries (APQ)

[Automatic Persisted Queries (**APQ**)](/apollo-server/performance/apq/) enable GraphQL clients to send a server the _hash_ of their query string, _instead of_ sending the query string itself. When query strings are very large, this can significantly reduce network usage.

The router supports using APQ in its communications with both clients _and_ subgraphs:

- **In its communications with clients,** the router acts as a GraphQL _server_, because it _receives_ queries from clients.
- **In its communications with subgraphs,** the router acts as a GraphQL _client_, because it _sends_ queries to subgraphs.

Because the router's role differs between these two interactions, you configure these APQ settings separately.

### APQ with clients

The router enables APQ caching for client operations by default. In your router's [YAML config file](/router/configuration/overview/#yaml-config-file), you can configure the maximum number of APQ entries in the cache like so:

```yaml title="router.yaml"
apq:
  router:
    cache:
      in_memory:
        limit: 512 # This is the default value.
```

You can also _disable_ client APQ support entirely like so:

```yaml title="router.yaml"
apq:
  enabled: false
```

### APQ with subgraphs

By default, the router does _not_ use APQ when sending queries to its subgraphs.

In your router's [YAML config file](/router/configuration/overview/#yaml-config-file), you can configure this APQ support with a combination of global and per-subgraph settings:

```yaml title="router.yaml"
apq:
  subgraph:
    # Disables subgraph APQ globally except where overridden per-subgraph
    all:
      enabled: false
    # Override global APQ setting for individual subgraphs
    subgraphs:
      products:
        enabled: true
```

In the example above, subgraph APQ is disabled _except for_ the `products` subgraph.
